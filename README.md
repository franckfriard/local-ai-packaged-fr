Voici la traduction compl√®te du fichier `README.md` en fran√ßais, en conservant le formatage Markdown original pour que vous puissiez le copier-coller directement.

---

# Package IA Auto-h√©berg√© (Self-hosted AI Package)

**Le Package IA Auto-h√©berg√©** est un mod√®le Docker Compose ouvert qui amorce rapidement un environnement de d√©veloppement **IA locale et Low Code** complet. Il inclut Ollama pour vos LLM locaux, Open WebUI pour une interface de chat avec vos agents n8n, et Supabase pour votre base de donn√©es, votre stockage vectoriel et l'authentification.

Ceci est la version de Cole avec quelques am√©liorations et l'ajout de Supabase, Open WebUI, Flowise, Neo4j, Langfuse, SearXNG et Caddy !
Des workflows d'agents IA RAG pr√©-construits (issus de la vid√©o) sont inclus dans `n8n/backup/workflows/` - voir [Importation des workflows de d√©marrage](https://www.google.com/search?q=%23importation-des-workflows-de-d%C3%A9marrage) pour les instructions de configuration.

**IMPORTANT** : Supabase a mis √† jour certaines variables d'environnement, vous devrez donc peut-√™tre ajouter de nouvelles valeurs par d√©faut dans votre `.env` (disponibles dans mon `.env.example`) si vous aviez d√©j√† ce projet en cours d'ex√©cution et que vous r√©cup√©rez les nouveaux changements. Plus pr√©cis√©ment, vous devez ajouter "POOLER_DB_POOL_SIZE=5" √† votre fichier `.env`. Ceci est requis si vous utilisiez le package avant le 14 juin.

## Liens Importants

* [Forum de la communaut√© Local AI](https://thinktank.ottomator.ai/c/local-ai/18) sur le oTTomator Think Tank.
* [Tableau Kanban GitHub](https://github.com/users/coleam00/projects/2/views/1) pour l'impl√©mentation des fonctionnalit√©s et la correction de bugs.
* [Kit de d√©marrage Local AI original](https://github.com/n8n-io/self-hosted-ai-starter-kit) par l'√©quipe n8n.
* T√©l√©chargez mon int√©gration N8N + OpenWebUI [directement sur le site Open WebUI.](https://openwebui.com/f/coleam/n8n_pipe/) (plus d'instructions ci-dessous).

S√©lectionn√© par [https://github.com/n8n-io](https://github.com/n8n-io) et [https://github.com/coleam00](https://github.com/coleam00), ce projet combine la plateforme n8n auto-h√©berg√©e avec une liste organis√©e de produits et composants IA compatibles pour d√©marrer rapidement la cr√©ation de workflows IA auto-h√©berg√©s.

### Ce qui est inclus

‚úÖ **[n8n Auto-h√©berg√©](https://n8n.io/)** - Plateforme Low-code avec plus de 400 int√©grations et des composants IA avanc√©s.

‚úÖ **[Supabase](https://supabase.com/)** - Base de donn√©es open source "as a service" - la base de donn√©es la plus utilis√©e pour les agents IA.

‚úÖ **[Ollama](https://ollama.com/)** - Plateforme LLM multi-plateforme pour installer et ex√©cuter les derniers LLM locaux.

‚úÖ **[Open WebUI](https://openwebui.com/)** - Interface de type ChatGPT pour interagir en priv√© avec vos mod√®les locaux et vos agents N8N.

‚úÖ **[Flowise](https://flowiseai.com/)** - Constructeur d'agents IA No/low code qui s'associe tr√®s bien avec n8n.

‚úÖ **[Qdrant](https://qdrant.tech/)** - Stockage vectoriel (Vector Store) open source haute performance avec une API compl√®te. Bien que vous puissiez utiliser Supabase pour le RAG, celui-ci a √©t√© conserv√© contrairement √† Postgres car il est plus rapide que Supabase et constitue parfois une meilleure option.

‚úÖ **[Neo4j](https://neo4j.com/)** - Moteur de graphe de connaissances qui propulse des outils comme GraphRAG, LightRAG et Graphiti.

‚úÖ **[SearXNG](https://searxng.org/)** - Moteur de m√©ta-recherche internet gratuit et open source qui agr√®ge les r√©sultats de jusqu'√† 229 services de recherche. Les utilisateurs ne sont ni suivis ni profil√©s, d'o√π la pertinence avec le package IA local.

‚úÖ **[Caddy](https://caddyserver.com/)** - HTTPS/TLS g√©r√© pour les domaines personnalis√©s.

‚úÖ **[Langfuse](https://langfuse.com/)** - Plateforme d'ing√©nierie LLM open source pour l'observabilit√© des agents.

## Pr√©requis

Avant de commencer, assurez-vous d'avoir install√© les logiciels suivants :

* [Python](https://www.python.org/downloads/) - Requis pour ex√©cuter le script d'installation.
* [Git/GitHub Desktop](https://desktop.github.com/) - Pour une gestion facile du d√©p√¥t.
* [Docker/Docker Desktop](https://www.docker.com/products/docker-desktop/) - Requis pour ex√©cuter tous les services.

## Installation

Clonez le d√©p√¥t et naviguez vers le r√©pertoire du projet :

```bash
git clone -b stable https://github.com/coleam00/local-ai-packaged.git
cd local-ai-packaged

```

Avant de lancer les services, vous devez configurer vos variables d'environnement pour Supabase en suivant leur [guide d'auto-h√©bergement](https://supabase.com/docs/guides/self-hosting/docker#securing-your-services).

1. Faites une copie de `.env.example` et renommez-la en `.env` dans le r√©pertoire racine du projet.
2. D√©finissez les variables d'environnement requises suivantes :
```bash
############
# Configuration N8N
############
N8N_ENCRYPTION_KEY=
N8N_USER_MANAGEMENT_JWT_SECRET=

############
# Secrets Supabase
############
POSTGRES_PASSWORD=
JWT_SECRET=
ANON_KEY=
SERVICE_ROLE_KEY=
DASHBOARD_USERNAME=
DASHBOARD_PASSWORD=
POOLER_TENANT_ID=

############
# Secrets Neo4j
############   
NEO4J_AUTH=

############
# Identifiants Langfuse
############

CLICKHOUSE_PASSWORD=
MINIO_ROOT_PASSWORD=
LANGFUSE_SALT=
NEXTAUTH_SECRET=
ENCRYPTION_KEY=  

```



> [!IMPORTANT]
> Assurez-vous de g√©n√©rer des valeurs al√©atoires s√©curis√©es pour tous les secrets. N'utilisez jamais les valeurs d'exemple en production.

3. D√©finissez les variables d'environnement suivantes si vous d√©ployez en production, sinon laissez-les comment√©es :
```bash
############
# Config Caddy
############

N8N_HOSTNAME=n8n.votredomaine.com
WEBUI_HOSTNAME=:openwebui.votredomaine.com
FLOWISE_HOSTNAME=:flowise.votredomaine.com
SUPABASE_HOSTNAME=:supabase.votredomaine.com
OLLAMA_HOSTNAME=:ollama.votredomaine.com
SEARXNG_HOSTNAME=searxng.votredomaine.com
NEO4J_HOSTNAME=neo4j.votredomaine.com
LETSENCRYPT_EMAIL=votre-adresse-email

```



---

Le projet inclut un script `start_services.py` qui g√®re le d√©marrage des services Supabase et de l'IA locale. Le script accepte un drapeau `--profile` pour sp√©cifier quelle configuration GPU utiliser.

### Pour les utilisateurs de GPU Nvidia

```bash
python start_services.py --profile gpu-nvidia

```

> [!NOTE]
> Si vous n'avez jamais utilis√© votre GPU Nvidia avec Docker auparavant, veuillez suivre les
> [instructions Docker pour Ollama](https://github.com/ollama/ollama/blob/main/docs/docker.mdx).

### Pour les utilisateurs de GPU AMD sous Linux

```bash
python start_services.py --profile gpu-amd

```

### Pour les utilisateurs Mac / Apple Silicon

Si vous utilisez un Mac avec un processeur M1 ou plus r√©cent, vous ne pouvez malheureusement pas exposer votre GPU √† l'instance Docker. Il existe deux options dans ce cas :

1. Ex√©cuter le kit de d√©marrage enti√®rement sur le CPU :
```bash
python start_services.py --profile cpu

```


2. Ex√©cuter Ollama sur votre Mac pour une inf√©rence plus rapide, et s'y connecter depuis l'instance n8n :
```bash
python start_services.py --profile none

```


Si vous souhaitez ex√©cuter Ollama sur votre Mac, consultez la [page d'accueil d'Ollama](https://ollama.com/) pour les instructions d'installation.

#### Pour les utilisateurs Mac ex√©cutant OLLAMA localement

Si vous ex√©cutez OLLAMA localement sur votre Mac (pas dans Docker), vous devez modifier la variable d'environnement `OLLAMA_HOST` dans la configuration du service n8n. Mettez √† jour la section `x-n8n` dans votre fichier Docker Compose comme suit :

```yaml
x-n8n: &service-n8n
  # ... autres configurations ...
  environment:
    # ... autres variables d'environnement ...
    - OLLAMA_HOST=host.docker.internal:11434

```

De plus, apr√®s avoir vu "Editor is now accessible via: https://www.google.com/search?q=http://localhost:5678/" :

1. Allez sur https://www.google.com/search?q=http://localhost:5678/home/credentials
2. Cliquez sur "Local Ollama service"
3. Changez l'URL de base pour "[http://host.docker.internal:11434/](https://www.google.com/search?q=http://host.docker.internal:11434/)"

### Pour tous les autres

```bash
python start_services.py --profile cpu

```

### L'argument environment

Le script **start-services.py** offre la possibilit√© de passer l'une des deux options pour l'argument environment, **private** (environnement par d√©faut) et **public** :

* **private :** vous d√©ployez la stack dans un environnement s√ªr, donc beaucoup de ports peuvent √™tre rendus accessibles sans avoir √† se soucier de la s√©curit√©.
* **public :** la stack est d√©ploy√©e dans un environnement public, ce qui signifie que la surface d'attaque doit √™tre r√©duite au minimum. Tous les ports sauf le 80 et le 443 sont ferm√©s.

La stack initialis√©e avec

```bash
   python start_services.py --profile gpu-nvidia --environment private

```

√©quivaut √† celle initialis√©e avec

```bash
   python start_services.py --profile gpu-nvidia

```

## D√©ploiement dans le Cloud

### Pr√©requis pour les √©tapes ci-dessous

* Machine Linux (de pr√©f√©rence Ubuntu) avec Nano, Git et Docker install√©s.

### √âtapes suppl√©mentaires

Avant d'ex√©cuter les commandes ci-dessus pour r√©cup√©rer le d√©p√¥t et tout installer :

1. Ex√©cutez les commandes en tant que root pour ouvrir les ports n√©cessaires :
* ufw enable
* ufw allow 80 && ufw allow 443
* ufw reload


---


**ATTENTION**
ufw ne prot√®ge pas les ports publi√©s par docker, car les r√®gles iptables configur√©es par docker sont analys√©es avant celles configur√©es par ufw. Il existe une solution pour modifier ce comportement, mais cela d√©passe le cadre de ce projet. Assurez-vous simplement que tout le trafic passe par le service Caddy via le port 443. Le port 80 ne doit √™tre utilis√© que pour rediriger vers le port 443.
---


2. Ex√©cutez le script **start-services.py** avec l'argument d'environnement **public** pour indiquer que vous allez ex√©cuter le package dans un environnement public. Le script s'assurera que tous les ports, sauf le 80 et le 443, sont ferm√©s, ex :

```bash
   python3 start_services.py --profile gpu-nvidia --environment public

```

3. Configurez les enregistrements A (DNS) chez votre fournisseur pour faire pointer vos sous-domaines (d√©finis dans le fichier .env pour Caddy) vers l'adresse IP de votre instance cloud.
Par exemple, un enregistrement A pour pointer n8n vers [IP de l'instance cloud] pour https://www.google.com/url?sa=E&source=gmail&q=n8n.votredomaine.com

**NOTE** : Si vous utilisez une machine cloud sans la commande "docker compose" disponible par d√©faut, comme une instance GPU Ubuntu sur DigitalOcean, ex√©cutez ces commandes avant de lancer start_services.py :

* DOCKER_COMPOSE_VERSION=$(curl -s [https://api.github.com/repos/docker/compose/releases/latest](https://api.github.com/repos/docker/compose/releases/latest) | grep 'tag_name' | cut -d" -f4)
* sudo curl -L "[https://www.google.com/search?q=https://github.com/docker/compose/releases/download/$](https://github.com/docker/compose/releases/download/$){DOCKER_COMPOSE_VERSION}/docker-compose-linux-x86_64" -o /usr/local/bin/docker-compose
* sudo chmod +x /usr/local/bin/docker-compose
* sudo mkdir -p /usr/local/lib/docker/cli-plugins
* sudo ln -s /usr/local/bin/docker-compose /usr/local/lib/docker/cli-plugins/docker-compose

## Importation des workflows de d√©marrage

Ce package inclut des workflows n8n pr√©-construits dans le dossier `n8n/backup/workflows/`. Pour les importer :

1. Ouvrez n8n √† l'adresse [http://localhost:5678/](https://www.google.com/search?q=http://localhost:5678/) (ou votre domaine personnalis√© si d√©ploy√© dans le cloud).
2. Allez dans votre liste de workflows et cliquez sur le menu √† trois points ou utilisez **Import from File** (Importer depuis un fichier).
3. S√©lectionnez les fichiers JSON du dossier `n8n/backup/workflows/` sur votre machine locale.

Pour des instructions d√©taill√©es, consultez la [documentation officielle d'import/export n8n](https://docs.n8n.io/workflows/export-import/).

> [!NOTE]
> Vous devrez cr√©er des identifiants (credentials) pour chaque workflow apr√®s l'importation. Voir l'√©tape 3 du D√©marrage rapide ci-dessous.

## ‚ö°Ô∏è D√©marrage rapide et utilisation

Le composant principal du kit de d√©marrage IA auto-h√©berg√© est un fichier docker compose pr√©-configur√© avec le r√©seau et le disque, donc il n'y a pas grand-chose d'autre √† installer. Apr√®s avoir termin√© les √©tapes d'installation ci-dessus, suivez les √©tapes ci-dessous pour commencer.

1. Ouvrez [http://localhost:5678/](https://www.google.com/search?q=http://localhost:5678/) dans votre navigateur pour configurer n8n. Vous n'aurez √† faire cela qu'une seule fois. Vous ne cr√©ez PAS de compte avec n8n ici, c'est seulement un compte local pour votre instance !
2. Importez un workflow depuis `n8n/backup/workflows/` (voir [Importation des workflows de d√©marrage](https://www.google.com/search?q=%23importation-des-workflows-de-d%C3%A9marrage)), puis ouvrez-le depuis votre liste de workflows.
3. Cr√©ez des identifiants pour chaque service :
URL Ollama : http://ollama:11434
Postgres (via Supabase) : utilisez DB, username et password du fichier .env. IMPORTANT : L'h√¥te (Host) est 'db' car c'est le nom du service ex√©cutant Supabase.
URL Qdrant : http://qdrant:6333 (La cl√© API peut √™tre n'importe quoi car cela tourne localement).
Google Drive : Suivez [ce guide de n8n](https://docs.n8n.io/integrations/builtin/credentials/google/).
N'utilisez pas localhost pour l'URI de redirection, utilisez simplement un autre domaine que vous poss√©dez, cela fonctionnera quand m√™me !
Alternativement, vous pouvez configurer des [d√©clencheurs de fichiers locaux (local file triggers)](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.localfiletrigger/).
4. S√©lectionnez **Test workflow** pour lancer le workflow.
5. Si c'est la premi√®re fois que vous ex√©cutez le workflow, vous devrez peut-√™tre attendre qu'Ollama finisse de t√©l√©charger Llama3.1. Vous pouvez inspecter les logs de la console docker pour v√©rifier la progression.
6. Assurez-vous d'activer le workflow et de copier l'URL du webhook de "Production" !
7. Ouvrez [http://localhost:3000/](https://www.google.com/search?q=http://localhost:3000/) dans votre navigateur pour configurer Open WebUI.
Vous n'aurez √† faire cela qu'une seule fois. Vous ne cr√©ez PAS de compte avec Open WebUI ici, c'est seulement un compte local pour votre instance !
8. Allez dans Workspace -> Functions -> Add Function -> Donnez un nom + description puis collez le code de `n8n_pipe.py`.
La fonction est √©galement [publi√©e ici sur le site d'Open WebUI](https://openwebui.com/f/coleam/n8n_pipe/).
9. Cliquez sur l'ic√¥ne d'engrenage et d√©finissez `n8n_url` avec l'URL de production du webhook que vous avez copi√©e √† l'√©tape pr√©c√©dente.
10. Activez la fonction et elle sera d√©sormais disponible dans votre menu d√©roulant de mod√®les en haut √† gauche !

Pour ouvrir n8n √† tout moment, visitez [http://localhost:5678/](https://www.google.com/search?q=http://localhost:5678/) dans votre navigateur.
Pour ouvrir Open WebUI √† tout moment, visitez [http://localhost:3000/](https://www.google.com/search?q=http://localhost:3000/).

Avec votre instance n8n, vous aurez acc√®s √† plus de 400 int√©grations et une suite de n≈ìuds IA basiques et avanc√©s tels que les n≈ìuds
[AI Agent](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/),
[Text classifier](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.text-classifier/),
et [Information Extractor](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.information-extractor/).
Pour tout garder en local, n'oubliez pas d'utiliser le n≈ìud Ollama pour votre mod√®le de langage et Qdrant comme stockage vectoriel.

> [!NOTE]
> Ce kit de d√©marrage est con√ßu pour vous aider √† d√©marrer avec des workflows IA auto-h√©berg√©s. Bien qu'il ne soit pas enti√®rement optimis√© pour des environnements de production, il combine des composants robustes qui fonctionnent bien ensemble pour des projets de preuve de concept (POC). Vous pouvez le personnaliser pour r√©pondre √† vos besoins sp√©cifiques.

## Mise √† jour (Upgrading)

Pour mettre √† jour tous les conteneurs vers leurs derni√®res versions (n8n, Open WebUI, etc.), ex√©cutez ces commandes :

```bash
# Arr√™ter tous les services
docker compose -p localai -f docker-compose.yml --profile <votre-profil> down

# R√©cup√©rer les derni√®res versions de tous les conteneurs
docker compose -p localai -f docker-compose.yml --profile <votre-profil> pull

# Red√©marrer les services avec votre profil souhait√©
python start_services.py --profile <votre-profil>

```

Remplacez `<votre-profil>` par l'un des suivants : `cpu`, `gpu-nvidia`, `gpu-amd`, ou `none`.

Note : Le script `start_services.py` lui-m√™me ne met pas √† jour les conteneurs - il les red√©marre simplement ou les t√©l√©charge si vous les t√©l√©chargez pour la premi√®re fois. Pour obtenir les derni√®res versions, vous devez ex√©cuter explicitement les commandes ci-dessus.

## D√©pannage (Troubleshooting)

Voici des solutions aux probl√®mes courants que vous pourriez rencontrer :

### Probl√®mes Supabase

* **Red√©marrage du Pooler Supabase** : Si le conteneur supabase-pooler red√©marre sans cesse, suivez les instructions dans [cette issue GitHub](https://github.com/supabase/supabase/issues/30210#issuecomment-2456955578).
* **√âchec du d√©marrage de Supabase Analytics** : Si le conteneur supabase-analytics ne d√©marre pas apr√®s avoir chang√© votre mot de passe Postgres, supprimez le dossier `supabase/docker/volumes/db/data`.
* **Si vous utilisez Docker Desktop** : Allez dans les param√®tres Docker et assurez-vous que "Expose daemon on tcp://localhost:2375 without TLS" est activ√©.
* **Service Supabase Indisponible** - Assurez-vous de ne pas avoir de caract√®re "@" dans votre mot de passe Postgres ! Si la connexion au conteneur kong fonctionne (les logs du conteneur indiquent qu'il re√ßoit des requ√™tes de n8n) mais que n8n dit qu'il ne peut pas se connecter, c'est g√©n√©ralement le probl√®me d'apr√®s ce que la communaut√© a partag√©. D'autres caract√®res pourraient aussi √™tre interdits, le symbole @ est juste celui dont je suis s√ªr !
* **Red√©marrage de SearXNG** : Si le conteneur SearXNG red√©marre sans cesse, ex√©cutez la commande "chmod 755 searxng" dans le dossier local-ai-packaged pour que SearXNG ait les permissions n√©cessaires pour cr√©er le fichier uwsgi.ini.
* **Fichiers non trouv√©s dans le dossier Supabase** - Si vous obtenez des erreurs concernant des fichiers manquants dans le dossier supabase/ comme .env, docker/docker-compose.yml, etc., cela signifie tr√®s probablement que vous avez eu une "mauvaise" r√©cup√©ration (pull) du d√©p√¥t GitHub Supabase lorsque vous avez ex√©cut√© le script start_services.py. Supprimez enti√®rement le dossier supabase/ dans le dossier Local AI Package et r√©essayez.

### Probl√®mes de support GPU

* **Support GPU Windows** : Si vous avez du mal √† faire fonctionner Ollama avec le support GPU sous Windows avec Docker Desktop :
1. Ouvrez les param√®tres Docker Desktop
2. Activez le backend WSL 2
3. Consultez la [documentation Docker GPU](https://docs.docker.com/desktop/features/gpu/) pour plus de d√©tails


* **Support GPU Linux** : Si vous avez du mal √† faire fonctionner Ollama avec le support GPU sous Linux, suivez les [instructions Docker Ollama](https://github.com/ollama/ollama/blob/main/docs/docker.md).

### Probl√®mes de n≈ìuds n8n

* **N≈ìuds Local File Trigger ou Execute Command non disponibles** : √Ä partir de n8n v2+, ces n≈ìuds sont d√©sactiv√©s par d√©faut pour la s√©curit√©. Pour les activer, d√©commentez `NODES_EXCLUDE=[]` dans la section `x-n8n` du fichier `docker-compose.yml` et red√©marrez n8n. Voir [Acc√©der aux fichiers locaux](https://www.google.com/search?q=%23acc%C3%A9der-aux-fichiers-locaux) pour des instructions d√©taill√©es.

## üëì Lectures recommand√©es

n8n regorge de contenu utile pour d√©marrer rapidement avec ses concepts d'IA et ses n≈ìuds. Si vous rencontrez un probl√®me, allez sur le [support](https://www.google.com/search?q=%23support).

* [Agents IA pour les d√©veloppeurs : de la th√©orie √† la pratique avec n8n](https://blog.n8n.io/ai-agents/)
* [Tutoriel : Construire un workflow IA dans n8n](https://docs.n8n.io/advanced-ai/intro-tutorial/)
* [Concepts Langchain dans n8n](https://docs.n8n.io/advanced-ai/langchain/langchain-n8n/)
* [D√©monstration des diff√©rences cl√©s entre agents et cha√Ænes](https://docs.n8n.io/advanced-ai/examples/agent-chain-comparison/)
* [Que sont les bases de donn√©es vectorielles ?](https://docs.n8n.io/advanced-ai/examples/understand-vector-databases/)

## üé• Guide vid√©o

* [Guide de Cole pour le Local AI Starter Kit](https://youtu.be/pOsO40HSbOo)

## üõçÔ∏è Plus de mod√®les IA

Pour plus d'id√©es de workflows IA, visitez la **[galerie officielle de mod√®les IA n8n](https://n8n.io/workflows/?categories=AI)**. Depuis chaque workflow, s√©lectionnez le bouton **Use workflow** pour importer automatiquement le workflow dans votre instance n8n locale.

### Apprendre les concepts cl√©s de l'IA

* [Chat Agent IA](https://n8n.io/workflows/1954-ai-agent-chat/)
* [Chat IA avec n'importe quelle source de donn√©es (utilisant aussi le workflow n8n)](https://n8n.io/workflows/2026-ai-chat-with-any-data-source-using-the-n8n-workflow-tool/)
* [Chat avec OpenAI Assistant (en ajoutant une m√©moire)](https://n8n.io/workflows/2098-chat-with-openai-assistant-by-adding-a-memory/)
* [Utiliser un LLM open-source (via HuggingFace)](https://n8n.io/workflows/1980-use-an-open-source-llm-via-huggingface/)
* [Chat avec des documents PDF utilisant l'IA (citation des sources)](https://n8n.io/workflows/2165-chat-with-pdf-docs-using-ai-quoting-sources/)
* [Agent IA capable de scraper des pages web](https://n8n.io/workflows/2006-ai-agent-that-can-scrape-webpages/)

### Mod√®les IA locaux

* [Assistant Code Fiscal](https://n8n.io/workflows/2341-build-a-tax-code-assistant-with-qdrant-mistralai-and-openai/)
* [D√©couper des documents en notes d'√©tude avec MistralAI et Qdrant](https://n8n.io/workflows/2339-breakdown-documents-into-study-notes-using-templating-mistralai-and-qdrant/)
* [Assistant Documents Financiers utilisant Qdrant et](https://n8n.io/workflows/2335-build-a-financial-documents-assistant-using-qdrant-and-mistralai/) [ Mistral.ai](http://mistral.ai/)
* [Recommandations de recettes avec Qdrant et Mistral](https://n8n.io/workflows/2333-recipe-recommendations-with-qdrant-and-mistral/)

## Trucs & Astuces

### Acc√©der aux fichiers locaux

Le kit de d√©marrage IA auto-h√©berg√© cr√©era un dossier partag√© (par d√©faut, situ√© dans le m√™me r√©pertoire) qui est mont√© sur le conteneur n8n et permet √† n8n d'acc√©der aux fichiers sur le disque. Ce dossier dans le conteneur n8n est situ√© √† `/data/shared` -- c'est le chemin que vous devrez utiliser dans les n≈ìuds qui interagissent avec le syst√®me de fichiers local.

**N≈ìuds qui interagissent avec le syst√®me de fichiers local**

* [Lire/√âcrire des fichiers depuis le disque (Read/Write Files from Disk)](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.filesreadwrite/)
* [D√©clencheur de fichier local (Local File Trigger)](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.localfiletrigger/)
* [Ex√©cuter une commande (Execute Command)](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.executecommand/)

**Activer les n≈ìuds Local File Trigger et Execute Command**

√Ä partir de n8n v2+, les n≈ìuds `Local File Trigger` et `Execute Command` sont d√©sactiv√©s par d√©faut pour des raisons de s√©curit√©. Pour les activer dans cet environnement local/auto-h√©berg√© :

1. Ouvrez `docker-compose.yml`
2. Trouvez la section `x-n8n` et d√©commentez la ligne `NODES_EXCLUDE` :
```yaml
x-n8n: &service-n8n
  image: n8nio/n8n:latest
  environment:
    # ... autres variables ...
    - NODES_EXCLUDE=[]

```


3. Red√©marrez le conteneur n8n :
```bash
docker compose -p localai -f docker-compose.yml --profile <votre-profil> up -d n8n

```



Voir [Changements majeurs n8n 2.0 (Breaking Changes)](https://docs.n8n.io/2-0-breaking-changes/#disable-executecommand-and-localfiletrigger-nodes-by-default) pour plus de d√©tails.

## üìú Licence

Ce projet (cr√©√© √† l'origine par l'√©quipe n8n, lien en haut du README) est sous licence Apache License 2.0 - voir le fichier [LICENSE](https://www.google.com/search?q=LICENSE) pour plus de d√©tails.
